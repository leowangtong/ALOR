<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="Active Learning via Vision-Language Model Adaptation with Open Data." />
  <meta property="og:description" content="" />
  <meta property="og:url" content="" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/ALOR.png" />
  <meta property="og:image:width" content="653" />
  <meta property="og:image:height" content="584" />


  <meta name="twitter:title" content="Active Learning via Vision-Language Model Adaptation with Open Data.">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/ALOR.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Active Learning, Vision Language Model, Retrieval Augmented Learning, Data Distribution">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Active Learning via Vision-Language Model Adaptation with Open Data.</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Active Learning via Vision-Language Model Adaptation with Open Data
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://dadadawt.github.io/leowang.github.io/" target="_blank">Tong Wang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://myownskyw7.github.io/" target="_blank">Jiaqi Wang</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://aimerykong.github.io/" target="_blank">Shu Kong</a><sup>1</sup><sup>,</sup><sup>3</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Macau<br></span>&nbsp;
              <span class="author-block"><sup>2</sup>Shanghai AI Lab<br></span>&nbsp;
              <span class="author-block"><sup>3</sup>Institute of Collaborative Innovation<br></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/leowangtong/ALOR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
<!--                   <h2><strong style="color: red; font-size: x-large;">CVPR 2025</strong></h2> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

    <!-- Paper overview -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Overview</h2>
            <div class="content has-text-justified">
              <p style="text-align: justify;">
                In this work, we propose leveraging VLM’s pretraining data by retrieving samples closely related to the downstream task, 
                using them to augment the task-specific data for AL. 
                As expected, incorporating this data into existing AL methods leads to significant performance improvements. 
                Given that our method exploits open-source VLM and open data, we refer to it as Active Learning with Open Resources (ALOR).
                <br>
                <br>
                we propose novel Tail First Sampling (TFS) strategy for AL, an embarrassingly simple yet effective method that prioritizes sampling data from underrepresented classes to label. 
                Extensive experiments on standard benchmark datasets demonstrate that our ALOR achieves state-of-the-art performance, 
                significantly surpassing existing methods.

              </p>
            </div>
          </div>
        </div>
      </div>
    </section>


    <section class="section hero is-light2">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Our Contributions</h2>
            <div class="content has-text-justified">

              <div class="item">
                <!-- Your image here -->
                <img src="static/images/ALOR.png" alt="1" style="width: 500px; height: auto; display: block; margin: 0 auto;"/>
                <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                  Our results reveal three major contributions: 
                  <ol>
                   <li>We study AL by embracing both a VLM and its pretraining data (instantiating the open data). 
                    Particularly, we present <b>retrieval-based data augmentation (RDA)</b> to retrieve VLM’s pretraining data relevant to the 
                      downstream task, greatly enhancing existing AL methods.</li>
                    
                    <li>We observe that the retrieved data follows imbalanced distributions, implying how the VLM is biased and how
                    unlabeled task-specific data is similarly imbalanced distributed. This insight motivates our simple yet
                    novel <b>Tail First Sampling (TFS)</b> strategy that prioritizes rare classes in data selection. 
                    Extensive experiments show that TFS outperforms prior AL methods.</li>
                      
                    <li>We rigorously compare different VLM adaptation approaches including finetuning (FT), contrastive tuning (CT), 
                    linear probing (LP) and prompt tuning (PT). We show that CT significantly outperforms the others, 
                    even when retrieved data is not used. Our final method (<b>ALOR</b>) assembles CT, RDA and TFS, 
                    achieving the state-of-the-art on five benchmarks.
                    </li>

                  </ol>

                </p>
              </div>

            </div>
          </div>
        </div>
      </div>
    </section>


    <!-- End paper overview -->



  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Challenges</h2>
          <h3 class="title is-4">Retrieved data has domain gaps and follows an imbalanced distribution, degrading finetuning performance</h3>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <img src="static/images/challenges.png" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/>
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                Left: we show that the retrieved data exhibits different visual patterns
                (styles, backgrounds, resolutions, semantics, etc.) compared to the downstream few-shot data.
                Right: the retrieved data follows an imbalanced distribution, where some classes
                naturally appear less frequently than others. We retrieve relevant images from <a href="https://laion.ai/blog/laion-400-open-dataset/" style="color: #3273dc;">LAION</a>
                dataset following <a href="https://shubhamprshr27.github.io/neglected-tails-of-vlms/"  style="color: #3273dc;">REAL</a>.
              </p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="section hero is-light1">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Solution</h2>
          <h3 class="title is-4">Stage-Wise retrieval-Augmented fineTuning (SWAT)</h3>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <img src="static/images/swat.png" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/>
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                Given a data annotation guideline consisting of few-shot annotated images,
                SWAT retrieves <b>open data</b> relevant to the downstream concepts (e.g., from VLM's pretraining dataset <a href="https://laion.ai/blog/laion-400-open-dataset/">LAION</a>),
                and then finetunes a pretrained VLM (e.g., <a href="https://github.com/mlfoundations/open_clip?tab=readme-ov-file">OpenCLIP</a>) following a stage-wise strategy:
                <ul>
                  <li>Stage 1: end-to-end finetuning on the mixed retrieved and few-shot data.</li>
                  <li>Stage 2: retraining the classifier solely on the balanced few-shot images.</li>
                </ul>
                SWAT effectively mitigates the domain gaps and imbalanced distribution of retrieved data,
                significantly outperforming previous FSR methods, as shown below.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">

        <div class="column is-four-fifths">
          <h2 class="title is-3">Results</h2>
          <h3 class="title is-4">RDA significantly boosts active learning methods</h3>
          <div class="content has-text-justified">
            <img src="static/images/compare_rda_2D_barchart_v4.png" alt="comparison with SOTA."
                style="height: auto; display: block; margin: 10px;">
              <p>
                For each method in each round, we report the averaged accuracy and standard deviation over three random 
                runs across five datasets. In Round-0, methods with RDA achieve 1.7× higher accuracy than without! 
                In the last round (Round-6), RDA helps each method obtain >8% accuracy gains
              </p>
          </div>
        </div>
      </div>
    </div>
  </section>




  <!-- Text-Image generation -->
  <section class="section hero is-light1">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!-- <h2 class="title is-3">Results</h2> -->
          <h3 class="title is-4">TFS achieves state-of-the-art AL performance</h3>
          <div class="content has-text-justified">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/sota.png" alt="1" style="width: 2000px; height: auto; display: block; margin: 0 auto;"/>
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                Results show that our TFS outperforms existing AL methods when using PT for VLM adaptation. 
                Moreover, as our final ALOR method, “TFS w/ CT” (with RDA) significantly boosts performance, 
                achieving ∼7 points gains (in both accuracy and macro F1 averaged on the five datasets) over “TFS w/ PT” and existing AL methods.
              </p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Prompt Inversion -->



  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">

        <div class="column is-four-fifths">
          <h3 class="title is-4">CT consistently outperforms other adaptation approaches</h3>
          <div class="content has-text-justified">
            <img src="static/images/ct_is_better.png" alt="comparison with SOTA."
                style="height: auto; display: block; margin: 10px;">
              <p>
                We run each combination of adaptation approach and AL method (without RDA) on the challenging Semi-Aves dataset for three random runs, 
                and report the mean accuracy after Round-6. 
                CT consistently outperforms other adaptation approaches regardless of what AL methods are used.
              </p>
          </div>
        </div>
      </div>
    </div>
  </section>

    <section class="section hero is-light3">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">

        <div class="column is-four-fifths">
          <h3 class="title is-4">Visualization of per-round (x-axis) per-class (y-axis) accuracies on semi-Aves benchmark</h3>
          <div class="content has-text-justified">
            <img src="static/images/visual_comparison_performance_v5.png" alt="comparison with SOTA."
                style="height: auto; display: block; margin: 10px;">
              <p>
                For each method, we sort its per-class accuracies in round-0 and track the accuracies over time. 
                Results of different methods with and without RDA are shown in the top and bottom rows, respectively. 
                When adopting RDA, all AL methods suffer from learning for under-represented classes, which have limited retrieved data compared to common classes. 
                However, it is worth noting that our TFS can quickly improve on the under-represented classes as it prioritizes sampling data for them 
                (see the quick improvements of under-represented classes pointed by the black circle). 
                Without RDA, all methods do not have notable patterns in accuracy change although they yield better accuracies than previous rounds. 
                This suggests the benefit of RDA that indicates how to sample data to improve on underrepresented classes and eventually enhance overall accuracy.
              </p>
          </div>
        </div>
      </div>
    </div>
  </section>



  <!-- Acknowledgement -->


  <!--BibTex citation -->
  <section class="section hero is-light1" id="BibTeX">
    <div class="container is-max-desktop content ">
      <h2 class="title">BibTeX</h2>
      <p> If you find our work useful, please consider citing our papers:</p>
      <pre><code>
@inproceedings{wang2025active
        }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->



  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
